{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Practice: From Transformers to Alignment\n",
    "### Learning Objectives:\n",
    "* Understand attention mechanisms through NumPy code\n",
    "* Build a simple transformer block\n",
    "* Predict next token using a pretrained LLM\n",
    "* Analyze hallucinations\n",
    "* Explore supervised fine-tuning logic\n",
    "* Understand how DPO works via preference modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Introduction\n",
    "\n",
    "In this notebook, we will use several important Python libraries:\n",
    "\n",
    "- **[NumPy](https://education.launchcode.org/data-analysis-curriculum/eda-with-pandas/reading/numpy-intro/index.html?utm_term=launchcode&utm_campaign=&utm_source=bing&utm_medium=ppc&hsa_acc=4368208516&hsa_cam=568518766&hsa_grp=1173180668353233&hsa_ad=&hsa_src=o&hsa_tgt=dat-2325123495982042:loc-190&hsa_kw=launchcode&hsa_mt=b&hsa_net=adwords&hsa_ver=3&msclkid=f69fad9bed3f18d44e31c5a6703d580b&utm_content=Group%202)**: The fundamental package for scientific computing with Python. We use it for matrix operations and to demonstrate the attention mechanism.\n",
    "- **[PyTorch](https://www.geeksforgeeks.org/start-learning-pytorch-for-beginners/)**: A popular deep learning framework. We use it to build and train neural network models, including transformer blocks.\n",
    "- **[Hugging Face Transformers](https://github.com/huggingface/transformers)**: Provides state-of-the-art pre-trained models and tools for natural language processing. We use it to load and interact with large language models (LLMs).\n",
    "- **[huggingface-cli](https://huggingface.co/docs/huggingface_hub/main/en/guides/cli)**: A command-line tool for managing Hugging Face models and datasets. Useful for downloading models or checking your authentication.\n",
    "\n",
    "Make sure you have these packages installed. You can install them using pip if needed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torch in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.7.1+cu126)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.7 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.7/41.7 kB 499.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.33.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2024.6.1)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 30.7/41.5 kB 7.6 kB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 30.7/41.5 kB 7.6 kB/s eta 0:00:02\n",
      "     --------------------------------------  41.0/41.5 kB 11.0 kB/s eta 0:00:01\n",
      "     --------------------------------------- 41.5/41.5 kB 11.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 30.7/41.7 kB 6.8 kB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 30.7/41.7 kB 6.8 kB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 30.7/41.7 kB 6.8 kB/s eta 0:00:02\n",
      "     --------------------------------------- 41.7/41.7 kB 10.0 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ------------------------------ --------- 30.7/40.9 kB 3.4 kB/s eta 0:00:03\n",
      "     ---------------------------------------- 40.9/40.9 kB 4.3 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.9 kB ? eta -:--:--\n",
      "     ----------------------------- --------- 30.7/40.9 kB 50.3 kB/s eta 0:00:01\n",
      "     --------------------------------------- 40.9/40.9 kB 72.7 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.9 kB ? eta -:--:--\n",
      "     ---------------------------- --------- 30.7/40.9 kB 100.9 kB/s eta 0:00:01\n",
      "     ---------------------------- --------- 30.7/40.9 kB 100.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 40.9/40.9 kB 103.3 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 40.2/40.2 kB 966.2 kB/s eta 0:00:00\n",
      "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.2 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 20.5/40.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 40.2/40.2 kB 239.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.0/44.0 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/44.4 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 726.6 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ------------------------------------ --- 41.0/44.4 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     --------------------------- ------------ 30.7/44.4 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 550.8 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 41.0/44.1 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 44.1/44.1 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.5/43.5 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/44.1 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.1/44.1 kB 736.8 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ------------------ --------------------- 20.5/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     --------------------------- ------------ 30.7/44.4 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 44.4/44.4 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 2.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.7/43.7 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.7/43.7 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.7 kB 108.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 267.3 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/43.7 kB 108.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 152.6 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/43.7 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 530.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------- ----------- 30.7/43.7 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 711.0 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 41.0/43.7 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 43.7/43.7 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/43.7 kB 487.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 428.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.6 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.6 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/43.6 kB 325.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.6/43.6 kB 306.3 kB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/61.0 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 61.0/61.0 kB 819.4 kB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 41.0/43.6 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 43.6/43.6 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/43.6 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.6/43.6 kB 428.0 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 43.6/43.6 kB 710.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 41.0/43.6 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 43.6/43.6 kB 1.0 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------- ----------- 30.7/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.8 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/43.8 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 30.7/43.8 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.8/43.8 kB 215.2 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/138.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/138.0 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/138.0 kB 660.6 kB/s eta 0:00:01\n",
      "     ---------------- -------------------- 61.4/138.0 kB 544.7 kB/s eta 0:00:01\n",
      "     --------------------- --------------- 81.9/138.0 kB 573.4 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 92.2/138.0 kB 476.3 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 122.9/138.0 kB 450.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 138.0/138.0 kB 454.6 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/138.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/138.0 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 41.0/138.0 kB 960.0 kB/s eta 0:00:01\n",
      "     -------------------------- ------------ 92.2/138.0 kB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- - 133.1/138.0 kB 983.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 138.0/138.0 kB 905.9 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/137.6 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/137.6 kB 653.6 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 81.9/137.6 kB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 112.6/137.6 kB 819.2 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 133.1/137.6 kB 871.5 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 133.1/137.6 kB 871.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 137.6/137.6 kB 478.7 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/134.8 kB 259.2 kB/s eta 0:00:01\n",
      "     -------- ---------------------------- 30.7/134.8 kB 259.2 kB/s eta 0:00:01\n",
      "     -------- ---------------------------- 30.7/134.8 kB 259.2 kB/s eta 0:00:01\n",
      "     -------- ---------------------------- 30.7/134.8 kB 259.2 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/134.8 kB 115.5 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/134.8 kB 115.5 kB/s eta 0:00:01\n",
      "     ---------------- -------------------- 61.4/134.8 kB 148.8 kB/s eta 0:00:01\n",
      "     ------------------- ----------------- 71.7/134.8 kB 151.0 kB/s eta 0:00:01\n",
      "     ------------------------- ----------- 92.2/134.8 kB 174.7 kB/s eta 0:00:01\n",
      "     ------------------------- ----------- 92.2/134.8 kB 174.7 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 102.4/134.8 kB 173.2 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 102.4/134.8 kB 173.2 kB/s eta 0:00:01\n",
      "     ------------------------------------ 134.8/134.8 kB 194.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/134.8 kB 660.6 kB/s eta 0:00:01\n",
      "     ---------------- -------------------- 61.4/134.8 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------------------- ----------- 92.2/134.8 kB 655.4 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 112.6/134.8 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 134.8/134.8 kB 666.3 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     ----------- --------------------------- 41.0/134.8 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 92.2/134.8 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 134.8/134.8 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     ----------------------- --------------- 81.9/134.8 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 134.8/134.8 kB 2.7 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "     ---------------------------------------- 0.0/130.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 122.9/130.7 kB 3.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 130.7/130.7 kB 2.6 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "     ---------------------------------------- 0.0/131.1 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 122.9/131.1 kB 7.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 131.1/131.1 kB 3.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "     ---------------------------------------- 0.0/131.1 kB ? eta -:--:--\n",
      "     ------------------ -------------------- 61.4/131.1 kB 1.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 122.9/131.1 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 131.1/131.1 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "     ---------------------------------------- 0.0/129.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 129.4/129.4 kB 3.8 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "     ---------------------------------------- 0.0/129.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 129.4/129.4 kB 3.7 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "     ---------------------------------------- 0.0/129.4 kB ? eta -:--:--\n",
      "     ------------ -------------------------- 41.0/129.4 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 122.9/129.4 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 129.4/129.4 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 112.6/126.8 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 126.8/126.8 kB 2.5 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/126.8 kB 960.0 kB/s eta 0:00:01\n",
      "     ------------------------- ------------- 81.9/126.8 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 126.8/126.8 kB 932.1 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/126.8 kB 660.6 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/126.8 kB 393.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 126.8/126.8 kB 826.3 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/123.5 kB ? eta -:--:--\n",
      "     ------------ ------------------------ 41.0/123.5 kB 653.6 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.5 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.5 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.5 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.5 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.5 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.5 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/123.5 kB 191.1 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/123.5 kB 191.1 kB/s eta 0:00:01\n",
      "     --------------------------- --------- 92.2/123.5 kB 174.7 kB/s eta 0:00:01\n",
      "     -----------------------------------  122.9/123.5 kB 211.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 123.5/123.5 kB 207.0 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.35.1-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 20.5/123.1 kB ? eta -:--:--\n",
      "     ------------ ------------------------ 41.0/123.1 kB 653.6 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/123.1 kB 653.6 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 51.2/123.1 kB 290.5 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 51.2/123.1 kB 290.5 kB/s eta 0:00:01\n",
      "     --------------------- --------------- 71.7/123.1 kB 245.8 kB/s eta 0:00:01\n",
      "     --------------------- --------------- 71.7/123.1 kB 245.8 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/123.1 kB 208.4 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/123.1 kB 208.4 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/123.1 kB 208.4 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 102.4/123.1 kB 190.4 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 102.4/123.1 kB 190.4 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 102.4/123.1 kB 190.4 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 102.4/123.1 kB 190.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 123.1/123.1 kB 164.2 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     --------- ----------------------------- 30.7/123.1 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/123.1 kB 667.8 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/123.1 kB 469.7 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/123.1 kB 512.0 kB/s eta 0:00:01\n",
      "     --------------------------- --------- 92.2/123.1 kB 403.5 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 112.6/123.1 kB 437.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 123.1/123.1 kB 401.5 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "     ---------------------------------------- 0.0/121.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/121.5 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/121.5 kB 660.6 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/121.5 kB 660.6 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/121.5 kB 326.8 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/121.5 kB 416.7 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/121.5 kB 374.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 121.5/121.5 kB 418.5 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "     ---------------------------------------- 0.0/121.5 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/121.5 kB 660.6 kB/s eta 0:00:01\n",
      "     ------------------------ ------------ 81.9/121.5 kB 919.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 121.5/121.5 kB 1.0 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/119.9 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/119.9 kB 435.7 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/119.9 kB 409.6 kB/s eta 0:00:01\n",
      "     ------------------------- ----------- 81.9/119.9 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/119.9 kB 438.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 119.9/119.9 kB 438.8 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     ------ ------------------------------ 20.5/119.9 kB 320.0 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/119.9 kB 393.8 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 51.2/119.9 kB 372.4 kB/s eta 0:00:01\n",
      "     ---------------------- -------------- 71.7/119.9 kB 357.2 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/119.9 kB 403.5 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 102.4/119.9 kB 368.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 119.9/119.9 kB 369.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/119.9 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/119.9 kB 435.7 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/119.9 kB 435.7 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/119.9 kB 217.9 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/119.9 kB 272.3 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/119.9 kB 272.3 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/119.9 kB 275.8 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/119.9 kB 275.8 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 112.6/119.9 kB 273.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 119.9/119.9 kB 269.6 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/119.9 kB 660.6 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/119.9 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------------ ------------------ 61.4/119.9 kB 656.4 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 112.6/119.9 kB 595.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 119.9/119.9 kB 586.7 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     ---------- ---------------------------- 30.7/118.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 81.9/118.5 kB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/118.5 kB 744.7 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 112.6/118.5 kB 652.2 kB/s eta 0:00:01\n",
      "     ------------------------------------ 118.5/118.5 kB 530.8 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/118.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/118.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/118.5 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/118.5 kB 131.3 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/118.5 kB 131.3 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/118.5 kB 131.3 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/118.5 kB 131.3 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 30.7/118.5 kB 131.3 kB/s eta 0:00:01\n",
      "     ------------- ------------------------ 41.0/118.5 kB 78.6 kB/s eta 0:00:01\n",
      "     ------------- ------------------------ 41.0/118.5 kB 78.6 kB/s eta 0:00:01\n",
      "     ------------- ------------------------ 41.0/118.5 kB 78.6 kB/s eta 0:00:01\n",
      "     ------------- ------------------------ 41.0/118.5 kB 78.6 kB/s eta 0:00:01\n",
      "     ------------- ------------------------ 41.0/118.5 kB 78.6 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 61.4/118.5 kB 81.9 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 61.4/118.5 kB 81.9 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 61.4/118.5 kB 81.9 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 81.9/118.5 kB 95.6 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 81.9/118.5 kB 95.6 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 81.9/118.5 kB 95.6 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 112.6/118.5 kB 114.9 kB/s eta 0:00:01\n",
      "     ------------------------------------ 118.5/118.5 kB 117.4 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "     ---------------------------------------- 0.0/116.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/116.9 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/116.9 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/116.9 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/116.9 kB 259.2 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/116.9 kB 217.9 kB/s eta 0:00:01\n",
      "     ----------------------------- ------- 92.2/116.9 kB 435.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 116.9/116.9 kB 454.0 kB/s eta 0:00:00\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     ---------- ---------------------------- 30.7/113.6 kB 1.4 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 61.4/113.6 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 113.6/113.6 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl.metadata (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     -------------- ------------------------- 41.0/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl.metadata (112 kB)\n",
      "     ---------------------------------------- 0.0/112.3 kB ? eta -:--:--\n",
      "     ---------------------------------- --- 102.4/112.3 kB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 112.3/112.3 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl.metadata (112 kB)\n",
      "     ---------------------------------------- 0.0/112.3 kB ? eta -:--:--\n",
      "     ---------------------------------- --- 102.4/112.3 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 112.3/112.3 kB 3.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.29.0-py3-none-any.whl.metadata (111 kB)\n",
      "     ---------------------------------------- 0.0/111.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 111.9/111.9 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
      "     ---------------------------------------- 0.0/110.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 110.0/110.0 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
      "     ---------------------------------------- 0.0/110.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 110.0/110.0 kB 3.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl.metadata (106 kB)\n",
      "     ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.7/106.7 kB 6.4 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.27.3-py3-none-any.whl.metadata (106 kB)\n",
      "     ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.7/106.7 kB 6.4 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.27.2-py3-none-any.whl.metadata (106 kB)\n",
      "     ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.7/106.7 kB 6.0 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.27.1-py3-none-any.whl.metadata (106 kB)\n",
      "     ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.7/106.7 kB 6.0 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.27.0-py3-none-any.whl.metadata (106 kB)\n",
      "     ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.7/106.7 kB 6.0 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n",
      "     ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 100.3/100.3 kB 5.6 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl.metadata (100 kB)\n",
      "     ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 100.3/100.3 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
      "     ---------------------------------------- 0.0/93.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 93.9/93.9 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl.metadata (90 kB)\n",
      "     ---------------------------------------- 0.0/90.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.5/90.5 kB 2.5 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl.metadata (88 kB)\n",
      "     ---------------------------------------- 0.0/88.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 88.7/88.7 kB 5.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.23.0-py3-none-any.whl.metadata (88 kB)\n",
      "     ---------------------------------------- 0.0/88.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 88.7/88.7 kB 4.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl.metadata (84 kB)\n",
      "     ---------------------------------------- 0.0/84.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.4/84.4 kB 4.6 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl.metadata (84 kB)\n",
      "     ---------------------------------------- 0.0/84.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.5/84.5 kB 4.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.22.0-py3-none-any.whl.metadata (84 kB)\n",
      "     ---------------------------------------- 0.0/84.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.2/84.2 kB 4.6 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl.metadata (81 kB)\n",
      "     ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 82.0/82.0 kB 4.8 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.21.2-py3-none-any.whl.metadata (81 kB)\n",
      "     ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 82.0/82.0 kB 4.8 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl.metadata (81 kB)\n",
      "     ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 82.0/82.0 kB 4.5 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.21.0-py3-none-any.whl.metadata (81 kB)\n",
      "     ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 82.0/82.0 kB 4.5 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n",
      "     ---------------------------------------- 0.0/77.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.3/77.3 kB 4.5 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.20.0-py3-none-any.whl.metadata (77 kB)\n",
      "     ---------------------------------------- 0.0/77.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.3/77.3 kB 4.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.19.4-py3-none-any.whl.metadata (73 kB)\n",
      "     ---------------------------------------- 0.0/73.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.4/73.4 kB 4.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.19.3-py3-none-any.whl.metadata (73 kB)\n",
      "     ---------------------------------------- 0.0/73.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.4/73.4 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl.metadata (73 kB)\n",
      "     ---------------------------------------- 0.0/73.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.2/73.2 kB 4.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl.metadata (73 kB)\n",
      "     ---------------------------------------- 0.0/73.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.2/73.2 kB 4.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.19.0-py3-none-any.whl.metadata (73 kB)\n",
      "     ---------------------------------------- 0.0/73.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.2/73.2 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
      "     ---------------------------------------- 0.0/70.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.3/70.3 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting sacremoses (from transformers)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl.metadata (67 kB)\n",
      "     ---------------------------------------- 0.0/67.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.9/67.9 kB 3.8 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.8/61.8 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.16.1-py3-none-any.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.8/61.8 kB 3.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.16.0-py3-none-any.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.8/61.8 kB 3.4 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.8/59.8 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.14.1-py3-none-any.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.7/59.7 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.13.0-py3-none-any.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.7/59.7 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.6/56.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.12.4-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.6/56.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.12.3-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.6/56.6 kB 2.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.12.2-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.6/56.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.12.1-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.6/56.6 kB 2.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.12.0-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.6/56.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n",
      "     ---------------------------------------- 0.0/53.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.7/53.7 kB 2.7 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.11.2-py3-none-any.whl.metadata (53 kB)\n",
      "     ---------------------------------------- 0.0/53.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.7/53.7 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.11.1-py3-none-any.whl.metadata (53 kB)\n",
      "     ---------------------------------------- 0.0/53.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.7/53.7 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.11.0-py3-none-any.whl.metadata (53 kB)\n",
      "     ---------------------------------------- 0.0/53.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.7/53.7 kB 2.7 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.10.3-py3-none-any.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.10.2-py3-none-any.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB 2.8 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.10.1-py3-none-any.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.9.2-py3-none-any.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.5/49.5 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.9.1-py3-none-any.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.5/49.5 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.9.0-py3-none-any.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.5/49.5 kB 2.6 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.8/48.8 kB 2.4 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.8/48.8 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.8.0-py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.8/48.8 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.7.0-py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.3/48.3 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl.metadata (45 kB)\n",
      "     ---------------------------------------- 0.0/45.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.4/45.4 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.6.0-py3-none-any.whl.metadata (45 kB)\n",
      "     ---------------------------------------- 0.0/45.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.4/45.4 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB ? eta 0:00:00\n",
      "  Downloading transformers-4.5.0-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading transformers-4.4.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.4.1-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.4.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.3.2-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.3.1-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.3.0-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.2.2-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.2.1-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.2.0-py3-none-any.whl.metadata (36 kB)\n",
      "  Downloading transformers-4.1.1-py3-none-any.whl.metadata (34 kB)\n",
      "  Downloading transformers-4.1.0-py3-none-any.whl.metadata (34 kB)\n",
      "  Downloading transformers-4.0.1-py3-none-any.whl.metadata (33 kB)\n",
      "  Downloading transformers-4.0.0-py3-none-any.whl.metadata (33 kB)\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl.metadata (32 kB)\n",
      "  Downloading transformers-3.5.0-py3-none-any.whl.metadata (32 kB)\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading transformers-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading transformers-3.2.0-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.2/49.2 kB ? eta 0:00:00\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-3.0.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-3.0.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
      "     ---------------------------------------- 0.0/45.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.3/45.3 kB 2.3 MB/s eta 0:00:00\n",
      "  Downloading transformers-2.10.0-py3-none-any.whl.metadata (45 kB)\n",
      "     ---------------------------------------- 0.0/45.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.2/45.2 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading transformers-2.9.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.7/44.7 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.9.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.5/43.5 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.7.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.2/43.2 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.6.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.1/43.1 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.4/42.4 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.5.0-py3-none-any.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.4/42.4 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.4.1-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.9/40.9 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.4.0-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.9/40.9 kB ? eta 0:00:00\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting boto3 (from transformers)\n",
      "  Downloading boto3-1.39.17-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Collecting sentencepiece (from transformers)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->transformers) (0.4.6)\n",
      "Collecting botocore<1.40.0,>=1.39.17 (from boto3->transformers)\n",
      "  Downloading botocore-1.39.17-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->transformers)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->transformers)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: click in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacremoses->transformers) (8.2.1)\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from botocore<1.40.0,>=1.39.17->boto3->transformers) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.17->boto3->transformers) (1.17.0)\n",
      "Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.4 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 276.5/447.4 kB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 447.4/447.4 kB 7.1 MB/s eta 0:00:00\n",
      "Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl (276 kB)\n",
      "   ---------------------------------------- 0.0/276.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 276.0/276.0 kB 17.7 MB/s eta 0:00:00\n",
      "Downloading boto3-1.39.17-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.9/139.9 kB 8.1 MB/s eta 0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ------------------------ -------------- 563.2/897.5 kB 17.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 897.5/897.5 kB 14.3 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------- ----- 839.7/991.5 kB 17.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 991.5/991.5 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading botocore-1.39.17-py3-none-any.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 16.5 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 1.0/13.9 MB 15.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.6/13.9 MB 12.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.6/13.9 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.6/13.9 MB 16.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.7/13.9 MB 17.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.4/13.9 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.6/13.9 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.4/13.9 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.8/13.9 MB 19.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.4/13.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.8/13.9 MB 17.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.5/13.9 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.5/13.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 19.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.3/13.9 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/13.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.3/85.3 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 307.7/307.7 kB 18.6 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, regex, joblib, jmespath, sacremoses, botocore, s3transfer, boto3, transformers\n",
      "Successfully installed boto3-1.39.17 botocore-1.39.17 jmespath-1.0.1 joblib-1.5.1 regex-2025.7.34 s3transfer-0.13.1 sacremoses-0.1.1 sentencepiece-0.2.0 transformers-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\johnny\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "\n",
      "positional arguments:\n",
      "  {download,upload,repo-files,env,login,whoami,logout,auth,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}\n",
      "                        huggingface-cli command helpers\n",
      "    download            Download files from the Hub\n",
      "    upload              Upload a file or a folder to a repo on the Hub\n",
      "    repo-files          Manage files in a repo on the Hub\n",
      "    env                 Print information about the environment.\n",
      "    login               Log in using a token from\n",
      "                        huggingface.co/settings/tokens\n",
      "    whoami              Find out which huggingface.co account you are logged\n",
      "                        in as.\n",
      "    logout              Log out\n",
      "    auth                Other authentication related commands\n",
      "    repo                {create} Commands to interact with your huggingface.co\n",
      "                        repos.\n",
      "    lfs-enable-largefiles\n",
      "                        Configure your repository to enable upload of files >\n",
      "                        5GB.\n",
      "    scan-cache          Scan cache directory.\n",
      "    delete-cache        Delete revisions from the cache directory.\n",
      "    tag                 (create, list, delete) tags for a repo in the hub\n",
      "    version             Print information about the huggingface-cli version.\n",
      "    upload-large-folder\n",
      "                        Upload a large folder to a repo on the Hub\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy torch transformers huggingface_hub\n",
    "\n",
    "# For `huggingface-cli`, it is included with `huggingface_hub` or `transformers`. You can check your installation with:\n",
    "\n",
    "! huggingface-cli --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Attention Mechanism (Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " [[0.472317   0.6004395  0.47280575 0.56079671 0.76212407 0.48117356\n",
      "  0.54135689 0.61600153]\n",
      " [0.46835529 0.61284489 0.47745831 0.557511   0.77170991 0.48230037\n",
      "  0.56134899 0.61996662]\n",
      " [0.47425451 0.61578046 0.48533681 0.56396424 0.76298441 0.46473933\n",
      "  0.53955355 0.61884739]\n",
      " [0.46978432 0.60429574 0.47276668 0.55957995 0.76502296 0.48343427\n",
      "  0.54745493 0.61506526]]\n",
      "Attention Weights:\n",
      " [[0.29168436 0.24899303 0.23584364 0.22347897]\n",
      " [0.31473761 0.23272291 0.24143685 0.21110263]\n",
      " [0.29163734 0.21916253 0.24068584 0.24851429]\n",
      " [0.29723171 0.24508877 0.23970565 0.21797388]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random Q, K, V matrices\n",
    "def generate_random_qkv(seq_len=4, d_model=8):\n",
    "    return [np.random.rand(seq_len, d_model) for _ in range(3)]\n",
    "\n",
    "# Scaled dot-product attention\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = softmax(scores)\n",
    "    output = np.dot(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "Q, K, V = generate_random_qkv()\n",
    "out, attn_weights = self_attention(Q, K, V)\n",
    "print(\"Attention Output:\\n\", out)\n",
    "print(\"Attention Weights:\\n\", attn_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Discussion: Walk students through the QK^T score computation, scaling, and softmax. Explain how this captures relationships between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Mini Transformer Block in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3437, -0.7434,  1.9965,  0.1187, -0.0828, -2.5316,  0.3967,\n",
      "           0.6023, -0.8104,  0.4995, -0.1412,  0.0738,  0.9935, -1.1261,\n",
      "           0.9544, -0.5436],\n",
      "         [ 0.5594,  1.2446, -1.4856, -1.0434, -0.1766, -0.1144,  0.9168,\n",
      "           1.5608, -0.6772,  0.0592, -0.8327, -1.6486, -0.4579,  1.4244,\n",
      "          -0.4341,  1.1052],\n",
      "         [ 0.3649,  0.0834,  0.4399,  0.8397,  0.9748, -1.9832, -0.1018,\n",
      "           0.7218, -0.5787,  0.6477,  1.4206, -0.4369,  1.0727, -1.9019,\n",
      "          -0.2366, -1.3265],\n",
      "         [-0.5305,  0.3394, -1.8503, -0.4006,  2.2120, -0.0298,  0.5067,\n",
      "           0.3052,  0.4162, -1.6105,  0.6547,  0.4939, -1.6868,  0.2873,\n",
      "           0.5142,  0.3790],\n",
      "         [-0.6227, -0.5340,  0.3867, -1.8154,  0.3300,  0.6576,  0.3610,\n",
      "          -0.6669,  0.6062,  0.8167,  1.9314, -0.4063,  1.3760, -1.1439,\n",
      "          -1.5919,  0.3156]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(1, 5, 16)  # batch_size=1, seq_len=5, embed_dim=16\n",
    "model = MiniTransformerBlock(embed_dim=16)\n",
    "out = model(x)\n",
    "print(out.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Goal: Show how self-attention and FFN work with residual and norm in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Next Token Prediction using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Use a Publicly Available Model\n",
    "Use a non-gated model such as TinyLlama or mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `MLE_Week2` has been saved to C:\\Users\\johnny\\.cache\\huggingface\\stored_tokens\n",
      "Your token has been saved to C:\\Users\\johnny\\.cache\\huggingface\\token\n",
      "Login successful.\n",
      "The current active token is: `MLE_Week2`\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Login via CLI\n",
    "#! pip install --upgrade huggingface_hub[cli]\n",
    "# To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
    "#! huggingface-cli login --token **removed**\n",
    "! hf auth login --token **removed**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:999: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5648ab98444bb9abf64f2520305e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login(token=\"your_hf_token\")  # optional if already logged in via CLI\n",
    "\n",
    "# you have to visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 to sign the agreement in order to use this model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a Mac with M1/M2/M3 and have this line working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "! python -c \"import torch; print(torch.backends.mps.is_available())\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it returns True, then you can run like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:999: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:29<00:00, 14.89s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is located in Paris, France, and is one of the most\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Optional if already logged in via CLI\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# Check device for MacBook (MPS if available, else CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load tokenizer and model with Hugging Face gated repo access\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True).to(device)\n",
    "\n",
    "# Prepare input prompt\n",
    "prompt = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run generation\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not sure about your GPU in your device, selects the best available device in the order: CUDA  MPS  CPU and also prints which one it chose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using CUDA (GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:999: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:20<00:00, 10.33s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated Output: The Eiffel Tower is located in Paris, France, and is one of the most\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Optional if already logged in via CLI\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# ------------------------------------------\n",
    "#  Device Selection: CUDA > MPS > CPU\n",
    "# ------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\" Using CUDA (GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Using CPU\")\n",
    "\n",
    "# ------------------------------------------\n",
    "#  Load Model from Hugging Face\n",
    "# ------------------------------------------\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32  # avoid FP16 on CPU\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------\n",
    "#  Prompt + Inference\n",
    "# ------------------------------------------\n",
    "prompt = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(\" Generated Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: DPO vs PPO  Side-by-Side Educational Example\n",
    "#### DPO: Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Loss: 2.06115369216775e-09\n",
      "DPO Loss: 8.194008692231508e-40\n",
      "DPO Loss: -0.0\n",
      "DPO Loss: 4.24835413113866e-18\n",
      "DPO Loss: 4.5398901420412585e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated log-probs of chosen vs rejected completions\n",
    "chosen_logp = torch.tensor([[-2.0]])\n",
    "rejected_logp = torch.tensor([[-4.0]])\n",
    "\n",
    "def dpo_loss(chosen_logp, rejected_logp, beta=0.1):\n",
    "    return -F.logsigmoid((chosen_logp - rejected_logp) / beta).mean()\n",
    "\n",
    "print(\"DPO Loss:\", dpo_loss(chosen_logp, rejected_logp).item())\n",
    "\n",
    "print(\"DPO Loss:\", dpo_loss(torch.tensor([[-1.0]]), torch.tensor([[-10.0]])).item())\n",
    "print(\"DPO Loss:\", dpo_loss(torch.tensor([[-2.0]]), torch.tensor([[-15.0]])).item())\n",
    "print(\"DPO Loss:\", dpo_loss(torch.tensor([[-3.0]]), torch.tensor([[-7.0]])).item())\n",
    "print(\"DPO Loss:\", dpo_loss(torch.tensor([[-4.0]]), torch.tensor([[-5.0]])).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO: Proximal Policy Optimization (simplified for in-class demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Loss: -1.2000000476837158\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated old and new policy log-probs (log _(a|s) and log __old(a|s))\n",
    "old_log_prob = torch.tensor([[-1.0]])  # from reference policy (e.g. GPT-4 before PPO step)\n",
    "new_log_prob = torch.tensor([[-0.8]])  # from updated policy\n",
    "reward = torch.tensor([[1.0]])         # reward from human or reward model\n",
    "epsilon = 0.2                          # PPO clipping parameter\n",
    "\n",
    "# Compute ratio of new to old policy\n",
    "log_ratio = new_log_prob - old_log_prob\n",
    "ratio = torch.exp(log_ratio)\n",
    "\n",
    "# Unclipped and clipped advantages\n",
    "advantage = reward  # assume reward ~ advantage for simplicity\n",
    "clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "# PPO loss (negative of the clipped surrogate objective)\n",
    "ppo_loss = -torch.min(ratio * advantage, clipped_ratio * advantage).mean()\n",
    "print(\"PPO Loss:\", ppo_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DPO vs PPO: Alignment Loss Comparison\n",
    "\n",
    "| Criterion         | DPO (Direct Preference Optimization)     | PPO (Proximal Policy Optimization)        |\n",
    "|------------------|-------------------------------------------|--------------------------------------------|\n",
    "|  Origin         | Preference modeling (UnfoldAI 2023)        | Reinforcement Learning (OpenAI 2017)       |\n",
    "|  Rejection Signal | Yes  uses chosen vs rejected pairs       | No  requires scalar reward                |\n",
    "|  Reward Signal   | Implicit via logit difference              | Explicit reward model needed               |\n",
    "|  Loss Function   | `-log(sigmoid((chosen - rejected)/))`     | `-min(ratio * A, clipped_ratio * A)`       |\n",
    "|  Optimization    | Binary classification over preferences     | Policy gradient with clipped surrogate     |\n",
    "|  Application     | DPO-tuned models like LLaMA 3              | RLHF-tuned models like InstructGPT         |\n",
    "|  Complexity      | Simpler (no reward model needed)           | More complex (needs reward model + sampling) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how aligning models toward human preference uses logit differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Inference with Quantization (O1 & O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d79aeff81944b13a376531ae07790a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "# Run model with torch_dtype=torch.float16 for O1\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept: Explain how FP16/O1 optimizes memory and speed at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Concept Breakdown: How FP16 & O1 Optimize Inference\n",
    "\n",
    "####  What is FP16?\n",
    "\n",
    "- **FP16** = *16-bit floating point*, also called **half precision**.\n",
    "- It uses **less memory** than FP32 (standard 32-bit float), with:\n",
    "  - 1 sign bit\n",
    "  - 5 exponent bits\n",
    "  - 10 mantissa bits\n",
    "- Typical FP32 values: `0.123456789`\n",
    "- FP16 representation: `0.1234` (lower precision but good enough for inference)\n",
    "\n",
    "---\n",
    "\n",
    "####  Why Use FP16?\n",
    "\n",
    "| Feature        | FP32                     | FP16                    |\n",
    "|----------------|--------------------------|-------------------------|\n",
    "| Memory usage   | 4 bytes per value         | 2 bytes per value       |\n",
    "| Compute speed  | Slower on GPUs            | Much faster on GPUs (especially A100/H100) |\n",
    "| Energy usage   | Higher                    | Lower                   |\n",
    "| Precision      | High                      | Slightly reduced (acceptable for inference) |\n",
    "\n",
    " FP16 helps run **large models** on GPUs with limited memory (e.g., 24GB vs 80GB cards).\n",
    "\n",
    "---\n",
    "\n",
    "####  What Is O1 Optimization?\n",
    "\n",
    "`O1` is a setting from **[DeepSpeed](https://www.deepspeed.ai/)** and **[Accelerate](https://huggingface.co/docs/accelerate)** used for **mixed-precision inference/training**.\n",
    "\n",
    "| Optimization Level | Description                           |\n",
    "|--------------------|---------------------------------------|\n",
    "| O0                 | Full precision (FP32)                 |\n",
    "| **O1**             | **Mixed precision (auto FP16 + FP32 fallback)** |\n",
    "| O2                 | Pure FP16                             |\n",
    "| O3                 | Advanced optimizations (e.g., quantization, kernel fusion) |\n",
    "\n",
    "#####  What Does O1 Do?\n",
    "- Automatically **casts compatible operations** (like matmul) to FP16\n",
    "- **Keeps numerically sensitive ops** (e.g., layer norm, softmax) in FP32\n",
    "- Result: **Best balance** between speed and stability\n",
    "\n",
    "---\n",
    "\n",
    "####  Benefits at Inference Time\n",
    "\n",
    "| Metric            | Before (FP32 / O0) | After (FP16 / O1) |\n",
    "|------------------|---------------------|--------------------|\n",
    "| VRAM usage       | High                | ~2x lower          |\n",
    "| Batch size limit | Smaller             | Larger             |\n",
    "| Latency          | Higher              | Lower              |\n",
    "| Throughput       | Lower               | Higher             |\n",
    "\n",
    "**Example:** Running LLaMA-7B in FP32 might require ~30GB VRAM, while FP16 can bring that down to ~16GB.\n",
    "\n",
    "---\n",
    "\n",
    "####  Code Example (Hugging Face Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files: 100%|| 3/3 [11:51<00:00, 237.02s/it]\n",
      "Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.53s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"openchat/openchat-3.5-1210\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  #  Enable half-precision\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bonus: Combine with O3\n",
    "O3 goes further with quantization, sparse attention, and custom kernels\n",
    "\n",
    "Supported by tools like DeepSpeed, vLLM, AWQ, and TensorRT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gguf\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gguf) (2.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gguf) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gguf) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\johnny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->gguf) (0.4.6)\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: gguf\n",
      "Successfully installed gguf-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gguf\n",
    "\n",
    "# working on llama-cpp-python install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GGUF model 'C:\\Users\\johnny\\python\\InferenceAI\\MLE_in_Gen_AI-Course\\p101\\llama-2-7b-chat.Q4_K_M.gguf' from 'TheBloke/Llama-2-7B-Chat-GGUF'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: TheBloke/Llama-2-7B-Chat-GGUF does not appear to have a file named C:\\Users\\johnny\\python\\InferenceAI\\MLE_in_Gen_AI-Course\\p101\\llama-2-7b-chat.Q4_K_M.gguf. Checkout 'https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main' for available files.\n",
      "Please ensure the model and GGUF file exist on the Hugging Face Hub or locally.\n",
      "\n",
      "Generating text...\n",
      "\n",
      "--- Generated Response ---\n",
      "Tell me a short story about a dragon.\n",
      "\n",
      "Once upon a time, in a faraway kingdom, there lived a dragon named Dracul. Dracul was different from other dragons. He didn't breathe fire, and he didn't want to conquer the kingdom. All he wanted was to live in peace and protect the people he cared about.\n",
      "\n",
      "One day, a group of villagers came to Dracul's cave, seeking his help. They were being terrorized by a band of go\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. Define the model and GGUF file name\n",
    "# We'll use a GGUF-specific model repository from TheBloke,\n",
    "# which is a great source for quantized models.\n",
    "model_id = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "gguf_file = \"C:\\\\Users\\\\johnny\\\\python\\\\InferenceAI\\\\MLE_in_Gen_AI-Course\\\\p101\\\\llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "# 2. Load the tokenizer and model from the GGUF file\n",
    "# The `gguf_file` parameter is the key to loading a GGUF model.\n",
    "try:\n",
    "    print(f\"Loading GGUF model '{gguf_file}' from '{model_id}'...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=gguf_file)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        gguf_file=gguf_file,\n",
    "        torch_dtype=torch.float32,  # GGUF models are dequantized to a PyTorch format for inference\n",
    "        device_map=\"auto\"           # Automatically map model layers to available devices (e.g., GPU, CPU)\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure the model and GGUF file exist on the Hugging Face Hub or locally.\")\n",
    "\n",
    "# 3. Use the model for inference (optional)\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    # Define your prompt\n",
    "    prompt = \"Tell me a short story about a dragon.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate a response\n",
    "    print(\"\\nGenerating text...\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n--- Generated Response ---\")\n",
    "    print(response)\n",
    "    print(\"--------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
